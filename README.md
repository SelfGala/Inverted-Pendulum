# CartPole-v1 Q-Learning Agent

使用 Q-learning 强化学习训练 OpenAI Gym 中的经典平衡控制任务 **CartPole-v1**。

## 环境简介

CartPole 是一个一阶倒立摆环境，其任务是通过移动小车来保持杆子竖直。

将该游戏的复杂情景在进行强化学习时抽解为三个变量：

**状态（State）**、**动作（Action）** 和 **奖励（Reward）**。

---

### 状态（State）

每个环境状态由以下 **4 个连续变量** 组成，表示当前物理系统的信息：

| 特征名称             | 含义                         | 数值范围           | 离散方式     |
|----------------------|------------------------------|--------------------|--------------|
| 小车位置 `cart_pos`  | 小车相对于中心的水平位置     | [-2.4, 2.4]        | 分为 4 段     |
| 小车速度 `cart_v`    | 小车的水平移动速度           | [-3.0, 3.0]        | 分为 4 段     |
| 杆子角度 `pole_angle`| 杆子与竖直方向的偏移角度     | [-0.5, 0.5]（弧度）| 分为 4 段     |
| 杆子角速度 `pole_v`  | 杆子旋转的角速度             | [-2.0, 2.0]        | 分为 4 段     |

对上述变量进行离散化，最终共有：

> \( 4 \times 4 \times 4 \times 4 = 256 \) 种可能状态。

---

### 动作（Action）

动作空间中，每个时间步只能执行以下两种动作之一：

| 动作编号 | 动作含义     |
|----------|--------------|
| `0`      | 向左施加力   |
| `1`      | 向右施加力   |

算法会根据当前状态的 Q 值表选择一个动作来与游戏环境交互。

---

### 奖励（Reward）

环境的奖励机制如下：

- 每撑过一个时间步（不失败）奖励 `+1`
- 若提前失败（杆子倾倒或小车出界），立即给予 `-200` 的惩罚奖励


## 算法：Q-Learning（离散化）

由于环境状态是连续的，代码中通过分箱方法将状态离散化为 256 个状态（4 个特征各分 4 个区间，\(4^4 = 256\) 种组合），进而构建一个 \(256 \times 2\) 的 Q 表来存储状态-动作对的估值。

### 状态离散化

每个状态由以下 4 个连续特征组成：

- **小车位置**（cart position）
- **小车速度**（cart velocity）
- **杆子角度**（pole angle）
- **杆子角速度**（pole velocity）

每个特征值均被分为 4 个区间后离散化，最终将 4 个离散值组合成一个 0 ～ 255 的整数。

## 控制策略：ε-贪婪方法

为平衡探索与利用，采用 ε-贪婪策略：

- **ε** 随着训练进程逐步衰减：  
  \(\epsilon = 0.5 \times 0.99^{\text{episode}}\)
- 在每次决策中，以 \(1-\epsilon\) 的概率选择当前 Q 表中最优动作，以 ε 的概率随机选择动作。

## 学习过程

- **学习率**：\(\alpha = 0.2\)
- **折扣因子**：\(\gamma = 0.99\)
- 每局游戏最多 **200** 步
- 总训练局数为 **1000** 局

## 奖励机制

- 正常情况下，每一步的奖励为 1 分。
- 如果提前失败（杆子掉倒或小车超出边界），给予 **-200** 的惩罚，帮助模型更快纠正不良决策。

## 成功条件与退出机制

当连续 100 局游戏的平均得分达到或超过 195 时，训练提前终止，并输出成功提示信息：

