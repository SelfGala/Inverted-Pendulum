<div align="center">

<h1>Single-Inverted-Pendulum å­¦ä¹ è®°å½•</h1>
<h1>Single Inverted Pendulum Learning Records</h1>

<p><strong>ğŸ¯ é¡¹ç›®æ¦‚è¿° / Project Overview</strong></p>
<p><i>1. å¼ºåŒ–å­¦ä¹ Q-learningç®—æ³•è§£å†³Gymçš„Cartpole-v1æ¨¡å‹</i></p>
<p><i>Q-learning reinforcement learning algorithm for solving Gym's Cartpole-v1 model</i></p>
<p><i>2. åŒç¯PIDæ§åˆ¶ä¸€é˜¶å€’ç«‹æ‘†Mujocoä»¿çœŸ</i></p>
<p><i>Double-loop PID control for single inverted pendulum Mujoco simulation</i></p>

<p><i>å…·ä½“ä»£ç åŠREADMEæ–‡ä»¶è§å„é¡¹ç›®æ–‡ä»¶å¤¹</i></p>
<p><i>For detailed code and README files, see respective project folders</i></p>

</div>

---

# 1ï¸âƒ£ Q-Learning å¼ºåŒ–å­¦ä¹  / Q-Learning Reinforcement Learning

ä½¿ç”¨ Q-learning å¼ºåŒ–å­¦ä¹ è®­ç»ƒ OpenAI Gym ä¸­çš„ç»å…¸å¹³è¡¡æ§åˆ¶ä»»åŠ¡ **CartPole-v1**ã€‚

*Training the classic balance control task **CartPole-v1** from OpenAI Gym using Q-learning reinforcement learning.*

**780 Episode å Cartpole è¡¨ç° / CartPole Performance After 780 Episodes:**

<p align="center">
  <img src="Photos/Cartpole.gif" width="400"/>
</p>

## ğŸŸ  ç¯å¢ƒç®€ä»‹ / Environment Introduction

CartPole æ˜¯ä¸€ä¸ªä¸€é˜¶å€’ç«‹æ‘†ç¯å¢ƒï¼Œå…¶ä»»åŠ¡æ˜¯é€šè¿‡ç§»åŠ¨å°è½¦æ¥ä¿æŒæ†å­ç«–ç›´ã€‚

*CartPole is a single inverted pendulum environment where the task is to keep the pole upright by moving the cart.*

å°†è¯¥æ¸¸æˆçš„å¤æ‚æƒ…æ™¯åœ¨è¿›è¡Œå¼ºåŒ–å­¦ä¹ æ—¶æŠ½è±¡ä¸ºä¸‰ä¸ªå˜é‡ï¼š

*The complex scenario of this game is abstracted into three variables for reinforcement learning:*

**çŠ¶æ€ï¼ˆStateï¼‰**ã€**åŠ¨ä½œï¼ˆActionï¼‰** å’Œ **å¥–åŠ±ï¼ˆRewardï¼‰**

***State**, **Action**, and **Reward***

---

### çŠ¶æ€ / State

æ¯ä¸ªç¯å¢ƒçŠ¶æ€ç”±ä»¥ä¸‹ **4 ä¸ªè¿ç»­å˜é‡** ç»„æˆï¼Œè¡¨ç¤ºå½“å‰ç‰©ç†ç³»ç»Ÿçš„ä¿¡æ¯ï¼š

*Each environment state consists of the following **4 continuous variables** representing the current physical system information:*

| ç‰¹å¾åç§°<br>*Feature Name* | å«ä¹‰<br>*Description* | æ•°å€¼èŒƒå›´<br>*Value Range* | ç¦»æ•£æ–¹å¼<br>*Discretization* |
|:--------------------:|:----------------------------:|:------------------:|:------------:|
| å°è½¦ä½ç½® `cart_pos`<br>*Cart Position* | å°è½¦ç›¸å¯¹äºä¸­å¿ƒçš„æ°´å¹³ä½ç½®<br>*Cart's horizontal position relative to center* | [-2.4, 2.4] | åˆ†ä¸º 4 æ®µ<br>*4 segments* |
| å°è½¦é€Ÿåº¦ `cart_v`<br>*Cart Velocity* | å°è½¦çš„æ°´å¹³ç§»åŠ¨é€Ÿåº¦<br>*Cart's horizontal velocity* | [-3.0, 3.0] | åˆ†ä¸º 4 æ®µ<br>*4 segments* |
| æ†å­è§’åº¦ `pole_angle`<br>*Pole Angle* | æ†å­ä¸ç«–ç›´æ–¹å‘çš„åç§»è§’åº¦<br>*Pole's deviation angle from vertical* | [-0.5, 0.5] (rad) | åˆ†ä¸º 4 æ®µ<br>*4 segments* |
| æ†å­è§’é€Ÿåº¦ `pole_v`<br>*Pole Angular Velocity* | æ†å­æ—‹è½¬çš„è§’é€Ÿåº¦<br>*Angular velocity of pole rotation* | [-2.0, 2.0] | åˆ†ä¸º 4 æ®µ<br>*4 segments* |

å¯¹ä¸Šè¿°å˜é‡è¿›è¡Œç¦»æ•£åŒ–ï¼Œæœ€ç»ˆå…±æœ‰ï¼š

*After discretizing the above variables, there are a total of:*

> 4 Ã— 4 Ã— 4 Ã— 4 = 256 ç§å¯èƒ½çŠ¶æ€ / *256 possible states*

---

### åŠ¨ä½œ / Action

åŠ¨ä½œç©ºé—´ä¸­ï¼Œæ¯ä¸ªæ—¶é—´æ­¥åªèƒ½æ‰§è¡Œä»¥ä¸‹ä¸¤ç§åŠ¨ä½œä¹‹ä¸€ï¼š

*In the action space, only one of the following two actions can be executed at each time step:*

| åŠ¨ä½œç¼–å·<br>*Action ID* | åŠ¨ä½œå«ä¹‰<br>*Action Description* |
|:--------:|:------------:|
| `0` | å‘å·¦æ–½åŠ åŠ›<br>*Apply force left* |
| `1` | å‘å³æ–½åŠ åŠ›<br>*Apply force right* |

ç®—æ³•ä¼šæ ¹æ®å½“å‰çŠ¶æ€çš„ Q å€¼è¡¨é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œæ¥ä¸æ¸¸æˆç¯å¢ƒäº¤äº’ã€‚

*The algorithm selects an action based on the Q-value table of the current state to interact with the game environment.*

---

### å¥–åŠ± / Reward

ç¯å¢ƒçš„å¥–åŠ±æœºåˆ¶å¦‚ä¸‹ï¼š

*The environment's reward mechanism is as follows:*

- æ¯æ’‘è¿‡ä¸€ä¸ªæ—¶é—´æ­¥ï¼ˆä¸å¤±è´¥ï¼‰å¥–åŠ± `+1`
  
  *Reward `+1` for each time step survived (without failure)*

- è‹¥æå‰å¤±è´¥ï¼ˆæ†å­å€¾å€’æˆ–å°è½¦å‡ºç•Œï¼‰ï¼Œç«‹å³ç»™äºˆ `-200` çš„æƒ©ç½šå¥–åŠ±
  
  *If failure occurs early (pole falls or cart goes out of bounds), immediately give `-200` penalty reward*

## ğŸŸ  Q-Learning å…¬å¼ï¼ˆæ›´æ–°Qè¡¨ï¼‰/ Q-Learning Formula (Q-Table Update)

Q-learning ç®—æ³•é€šè¿‡ä»¥ä¸‹å…¬å¼ä¸æ–­æ›´æ–°çŠ¶æ€-åŠ¨ä½œå‡½æ•° Q(s, a)ï¼š

*Q-learning algorithm continuously updates the state-action function Q(s, a) through the following formula:*

$$
Q(s_t, a_t) \leftarrow (1 - \alpha) \cdot Q(s_t, a_t) + \alpha \cdot \left[ r_t + \gamma \cdot \max_{a'} Q(s_{t+1}, a') \right]
$$

### ç¬¦å·å«ä¹‰è¯´æ˜ / Symbol Definitions

| ç¬¦å·<br>*Symbol* | å«ä¹‰è¯´æ˜<br>*Description* |
|:-----------------------:|:--------------------------------------------------:|
| `Q(s, a)` | å½“å‰çŠ¶æ€ `s` ä¸‹ï¼Œæ‰§è¡ŒåŠ¨ä½œ `a` çš„ Q å€¼<br>*Q-value for executing action `a` in current state `s`* |
| `Î±` (alpha) | å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ–°æ—§ä¿¡æ¯çš„æ›´æ–°æ¯”ä¾‹<br>*Learning rate, controlling the update ratio of new/old information* |
| `r` | å½“å‰æ­¥è·å¾—çš„å¥–åŠ±<br>*Reward obtained at current step* |
| `Î³` (gamma) | æŠ˜æ‰£å› å­ï¼Œè¡¡é‡æœªæ¥å¥–åŠ±çš„é‡è¦æ€§<br>*Discount factor, measuring the importance of future rewards* |
| `max_a' Q(s', a')` | ä¸‹ä¸€ä¸ªçŠ¶æ€ `s'` ä¸‹æ‰€æœ‰å¯èƒ½åŠ¨ä½œä¸­çš„æœ€å¤§ Q å€¼<br>*Maximum Q-value among all possible actions in next state `s'`* |

æ­¤æ›´æ–°è§„åˆ™å°†å½“å‰çš„ Q è¡¨ï¼Œé€šè¿‡ä¸æ–­å°è¯•å’Œæ›´æ–°ï¼Œæœ€ç»ˆå­¦å¾—æœ€ä¼˜ç­–ç•¥ã€‚

*This update rule allows the current Q-table to eventually learn the optimal policy through continuous trial and update.*

## ğŸŸ  æ§åˆ¶ç­–ç•¥ï¼šÎµ-è´ªå©ª / Control Strategy: Îµ-Greedy

ä¸ºå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œé‡‡ç”¨ Îµ-è´ªå©ªç­–ç•¥ï¼š

*To balance exploration and exploitation, an Îµ-greedy strategy is adopted:*

- **Îµ** éšç€è®­ç»ƒè¿›ç¨‹é€æ­¥è¡°å‡ / ***Îµ** gradually decays with training progress*:

$$
Îµ = 0.5 Ã— (0.99^n), \quad (n \text{ ä¸º Episode / is Episode})
$$

- åœ¨æ¯æ¬¡å†³ç­–ä¸­ï¼Œä»¥ `1-Îµ` çš„æ¦‚ç‡é€‰æ‹©å½“å‰ Q è¡¨ä¸­æœ€ä¼˜åŠ¨ä½œï¼Œä»¥ `Îµ` çš„æ¦‚ç‡éšæœºé€‰æ‹©åŠ¨ä½œã€‚

  *In each decision, select the optimal action from the current Q-table with probability `1-Îµ`, and select a random action with probability `Îµ`.*

## ğŸŸ  å­¦ä¹ è¿‡ç¨‹ã€æˆåŠŸæ¡ä»¶ä¸é€€å‡ºæœºåˆ¶ / Learning Process, Success Criteria & Exit Mechanism

- **å­¦ä¹ ç‡ / Learning Rate**ï¼š`Î± = 0.2`
- **æŠ˜æ‰£å› å­ / Discount Factor**ï¼š`Î³ = 0.99`
- æ¯å±€æ¸¸æˆæœ€å¤š `200` æ­¥ / *Maximum `200` steps per game*
- æ€»è®­ç»ƒå±€æ•°ä¸º `1000` å±€ / *Total training episodes: `1000`*
- å½“è¿ç»­ 100 å±€æ¸¸æˆçš„å¹³å‡å¾—åˆ†è¾¾åˆ°æˆ–è¶…è¿‡ `195` æ—¶ï¼Œè®­ç»ƒæå‰ç»ˆæ­¢ï¼Œå¹¶è¾“å‡ºæˆåŠŸæç¤ºä¿¡æ¯

  *When the average score of 100 consecutive games reaches or exceeds `195`, training terminates early with success notification*

---

# 2ï¸âƒ£ åŒç¯PIDæ§åˆ¶Mujocoä»¿çœŸ / Double-loop PID Control with Mujoco Simulation

ä½¿ç”¨åŒç¯ï¼ˆè§’åº¦+ä½ç½®ï¼‰PID æ§åˆ¶å™¨ä¸ DeepMind Control Suite çš„ Mujoco å€’ç«‹æ‘†æ¨¡å‹å®ç°ä»¿çœŸ

*Implementing simulation using double-loop (angle + position) PID controller with DeepMind Control Suite's Mujoco inverted pendulum model*

è°ƒæ•´åŒé—­ç¯PIDæ§åˆ¶å™¨å‚æ•°åCartpoleè¡¨ç°ï¼š

*CartPole performance after tuning double closed-loop PID controller parameters:*

<p align="center">
  <img src="https://raw.githubusercontent.com/SelfGala/Inverted-Pendulum/main/Photos/dm_control_cartpole.gif" width="400"/>
</p>

## ğŸŸ  æ•°å­¦å»ºæ¨¡ï¼šä¸€é˜¶å€’ç«‹æ‘†åŠ¨åŠ›å­¦æ¨¡å‹ / Mathematical Modeling: Single Inverted Pendulum Dynamics

ä»¥æ ‡å‡†çš„ **ä¸€é˜¶å€’ç«‹æ‘†ï¼ˆSingle Inverted Pendulum on a Cartï¼‰** ä¸ºç ”ç©¶å¯¹è±¡ï¼Œä¸‹å›¾ä¸º**å‚è€ƒä¸€é˜¶å€’ç«‹æ‘†**æ¨¡å‹ï¼š

*Taking the standard **Single Inverted Pendulum on a Cart** as the research object, the figure below shows the **reference single inverted pendulum** model:*

<p align="center">
  <img src="Photos/Cartpole-math-model.jpg" width="500"/>
</p>

### ç³»ç»Ÿå‚æ•° / System Parameters

| ç¬¦å·<br>*Symbol* | å«ä¹‰<br>*Description* | å•ä½<br>*Unit* | dm_control-xmlæ¨¡å‹å‚æ•°<br>*dm_control-xml Model Parameters* |
|:----------:|:------------------:|:-------------:|:---------------------:|
| `L` | æ‘†æ†é•¿åº¦<br>*Pendulum length* | ç±³ (m) | 0.6 |
| `m` | æ‘†æ†è´¨é‡<br>*Pendulum mass* | åƒå…‹ (kg) | 4.2 |
| `M` | å°è½¦è´¨é‡<br>*Cart mass* | åƒå…‹ (kg) | 8 |
| `g` | é‡åŠ›åŠ é€Ÿåº¦<br>*Gravitational acceleration* | ç±³/ç§’Â² (m/sÂ²) | -9.81 |
| `b` | æ‘©æ“¦é˜»å°¼ç³»æ•°<br>*Friction damping coefficient* | ç‰›Â·ç§’/ç±³ (NÂ·s/m) | 0.1 |

### ç³»ç»ŸçŠ¶æ€å˜é‡ / System State Variables

| ç¬¦å·<br>*Symbol* | å«ä¹‰<br>*Description* | å•ä½<br>*Unit* |
|:-----------------:|:--------------------------:|:----------------:|
| `x` | å°è½¦ä½ç½®<br>*Cart position* | ç±³ (m) |
| `ğ‘¥Ì‡` (x_dot) | å°è½¦é€Ÿåº¦<br>*Cart velocity* | ç±³/ç§’ (m/s) |
| `Î¸` (theta) | æ‘†æ†åç¦»ç«–ç›´çš„è§’åº¦<br>*Pole deviation angle from vertical* | å¼§åº¦ (rad) |
| `Î¸Ì‡` (theta_dot) | æ‘†æ†è§’é€Ÿåº¦<br>*Pole angular velocity* | å¼§åº¦/ç§’ (rad/s) |

å…¶åŠ¨åŠ›å­¦æ–¹ç¨‹å¯ç”±æ‹‰æ ¼æœ—æ—¥æ–¹ç¨‹æ¨å¯¼ï¼š

*The dynamics equations can be derived from Lagrangian equations:*

**æ°´å¹³æ–¹å‘ / Horizontal Direction** è¿åŠ¨æ–¹ç¨‹ / *Motion Equation*ï¼š

$$
F = (M + m)Â·áº + bÂ·áº‹ + mÂ·lÂ·Î¸ÌˆÂ·cos(Î¸) âˆ’ mÂ·lÂ·(Î¸Ì‡)Â²Â·sin(Î¸)
$$

**ç«–ç›´æ–¹å‘ / Vertical Direction** è¿åŠ¨æ–¹ç¨‹ / *Motion Equation*ï¼š

$$
P - mÂ·g = -mÂ·lÂ·Î¸ÌˆÂ·sin(Î¸) - mÂ·lÂ·(Î¸Ì‡)Â²Â·cos(Î¸)
$$

å¯¹ä¸¤ä¸ªè¿åŠ¨æ–¹ç¨‹è¿›è¡Œè¿‘ä¼¼å¤„ç†ã€çº¿æ€§åŒ–å¤„ç†ï¼Œcos(Î¸)â‰ˆ1ï¼Œsin(Î¸)â‰ˆÎ¸ï¼›å†è¿›è¡Œæ‹‰æ™®æ‹‰æ–¯å˜æ¢ï¼Œå¾—åˆ°ï¼š

*After approximation and linearization of the two motion equations, where cos(Î¸)â‰ˆ1, sin(Î¸)â‰ˆÎ¸, and applying Laplace transform:*

<p align="center">
  <img src="Photos/Laplace.png" width="350"/>
</p>

ç”±æ‹‰æ™®æ‹‰æ–¯å˜æ¢è§£å‡ºä¸¤ä¸ªæ–¹å‘çš„ä¼ é€’å‡½æ•°ï¼Œæ§åˆ¶ç³»ç»Ÿçš„çŠ¶æ€ç©ºé—´æ–¹ç¨‹å¯å†™æˆå¦‚ä¸‹å½¢å¼ï¼š

*From the Laplace transform, the transfer functions in both directions are solved, and the state space equation of the control system can be written as:*

$$
áº‹=AX+Bu
$$

$$
Y=CX+Du
$$

> uè¡¨ç¤ºç³»ç»Ÿæ§åˆ¶è¾“å…¥å‘é‡ï¼Œxè¡¨ç¤ºç³»ç»ŸçŠ¶æ€å˜é‡ï¼Œyè¡¨ç¤ºç³»ç»Ÿçš„è¾“å‡ºå‘é‡ï¼ŒAè¡¨ç¤ºç³»ç»Ÿçš„çŠ¶æ€çŸ©é˜µï¼ŒBè¡¨ç¤ºç³»ç»Ÿæ§åˆ¶è¾“å…¥çŸ©é˜µï¼ŒCè¡¨ç¤ºç³»ç»Ÿè¾“å‡ºè§‚æµ‹çŸ©é˜µï¼ŒDè¡¨ç¤ºç³»ç»Ÿè¾“å…¥è¾“å‡ºçŸ©é˜µã€‚
> 
> *u represents the system control input vector, x represents the system state variables, y represents the system output vector, A represents the system state matrix, B represents the system control input matrix, C represents the system output observation matrix, D represents the system input-output matrix.*

æ ¹æ®è¿åŠ¨æ–¹ç¨‹ç»„å’Œæ‹‰æ™®æ‹‰æ–¯å˜æ¢å¯¹áºå’ŒÎ¸Ìˆæ±‚è§£å¯å¾—è§£å¦‚ä¸‹ï¼š

*According to the motion equation system and Laplace transform, solving for áº and Î¸Ìˆ yields the following solutions:*

<p align="center">
  <img src="Photos/Solution_X.png" width="500"/>
</p>

<p align="center">
  <img src="Photos/Solution_Y.png" width="250"/>
</p>

## ğŸŸ  PIDæ§åˆ¶å™¨è®¾è®¡ / PID Controller Design

PIDæ§åˆ¶å™¨çš„å‚æ•°`Kp,Ki,Kd`é€šè¿‡ç»éªŒæ•°æ®æˆ–è¯•å‡‘æ³•å¾ˆéš¾è¿›è¡Œè°ƒæ•´å¾—åˆ°åˆé€‚çš„æ§åˆ¶å™¨ï¼Œæˆ‘ä»¬åœ¨å¾—åˆ°ä¼ é€’å‡½æ•°ä¹‹åï¼Œå¯ä»¥é€šè¿‡matlabçš„`pidtune`å‡½æ•°æ¥å¯¹å‚æ•°è¿›è¡Œä¸€ä¸ªç®€å•çš„è®¾è®¡ã€‚

*The PID controller parameters `Kp, Ki, Kd` are difficult to adjust through empirical data or trial-and-error methods to obtain a suitable controller. After obtaining the transfer function, we can use MATLAB's `pidtune` function for simple parameter design.*

ä¾‹å¦‚æˆ‘ä»¬æ ¹æ®dm_controlçš„cartpole-xmlæ¨¡å‹çš„ç‰©ç†å‚æ•°å¯ä»¥å¾—åˆ°å…¶è§’åº¦çš„ä¼ é€’å‡½æ•°ï¼š

*For example, based on the physical parameters of the dm_control cartpole-xml model, we can obtain its angle transfer function:*

$$
P_Î¸ = Î¸_s / U_s = \frac{0.276Â·sÂ²}{sâ´ + 0.11Â·sÂ³ âˆ’ 33.06Â·sÂ² âˆ’ 2.694Â·s}
$$

å¯¹å…¶åº”ç”¨`pidtune`: [Kpid, ~, ~] = pidtune(P_angle, 'PID')ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š

*Applying `pidtune`: [Kpid, ~, ~] = pidtune(P_angle, 'PID'), we obtain:*

- K_p = 449.4918
- K_i = 801.4594
- K_d = 56.3668

é€šè¿‡æ„å»ºä»¿çœŸé—­ç¯ç³»ç»Ÿçš„bodeå›¾ã€å•ä½é˜¶è·ƒå“åº”ã€å•ä½è„‰å†²å“åº”ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨ä¸€å®šæ—¶é—´åï¼Œè¾“å‡ºå·²ç»æ”¶æ•›ï¼š

*By constructing the Bode plot, unit step response, and unit impulse response of the simulated closed-loop system, we can see that the output has converged after a certain time:*

<p align="center">
  <img src="Photos/Bode_angle.png" width="1100"/>
</p>

## ğŸŸ  dm_control Mujocoä»¿çœŸè®¾è®¡ / dm_control Mujoco Simulation Design

æˆ‘ä»¬åº”ç”¨Deepmindæ„å»ºçš„Cartpoleæ¨¡å‹æ¥å¯¹å€’ç«‹æ‘†è¿›è¡Œä»¿çœŸï¼Œå…·ä½“xmlæ¨¡å‹æ–‡ä»¶è¯·è§æ–‡ä»¶å¤¹ï¼Œå…¶ä¸­éœ€è¦æ³¨æ„çš„æ˜¯xmlé»˜è®¤è´¨é‡æ ¹æ®ç‰©ä½“ä½“ç§¯è®¡ç®—ï¼Œé»˜è®¤å¯†åº¦ä¸ºæ°´çš„å¯†åº¦ï¼š1000kg/mÂ³

*We use the CartPole model built by DeepMind for inverted pendulum simulation. For specific xml model files, please see the folder. Note that xml default mass is calculated based on object volume, with default density being water density: 1000kg/mÂ³*

dm_control-Cartpoleé»˜è®¤å¼•ç”¨æ–¹æ³•å…·ä½“è§æ–‡ä»¶å¤¹ï¼Œæ¨¡å‹å‚æ•°è¯´æ˜è§ä¸‹ï¼š

*For dm_control-CartPole default usage methods, see the folder. Model parameter descriptions are as follows:*

### æ¨¡å‹ç»“æ„æ‘˜è¦ / Model Structure Summary

| å…ƒç´ <br>*Element* | åç§°/å±æ€§<br>*Name/Attribute* | ç±»å‹<br>*Type* | è¯´æ˜<br>*Description* |
|------------|------------------|--------------|----------------------------------|
| `cart` | Joint: `slider` | slide joint | å°è½¦æ²¿ X æ–¹å‘æ»‘åŠ¨ï¼ŒèŒƒå›´ Â±1 m<br>*Cart slides along X direction, range Â±1 m* |
| `pole` | Joint: `hinge` | hinge joint | æ‘†æ†ç»• Y è½´æ—‹è½¬ï¼ˆå‚ç›´å¹³é¢å†…ï¼‰<br>*Pole rotates around Y axis (in vertical plane)* |
| `floor` | Geom | plane | åœ°é¢ï¼ˆé™æ€ï¼‰<br>*Ground (static)* |
| `cpole` | Geom | capsule | æ‘†æ†å½¢çŠ¶ï¼Œ0.6m é•¿åº¦<br>*Pole shape, 0.6m length* |
| `cart` | Geom | box | å°è½¦ä¸»ä½“ï¼Œé•¿ 0.2m å®½ 0.1m é«˜ 0.05m<br>*Cart body, 0.2m Ã— 0.1m Ã— 0.05m* |
| `mocap1` / `mocap2` | Body | mocap | å¯è§†å‚è€ƒç‰©ä½“ï¼Œä¸å‚ä¸æ§åˆ¶<br>*Visual reference objects, not involved in control* |

---

### æ§åˆ¶ä¸æ‰§è¡Œæœºåˆ¶ / Control and Actuation Mechanism

| ç»„ä»¶ç±»å‹<br>*Component Type* | åç§°<br>*Name* | å±æ€§<br>*Attribute* | è¯´æ˜<br>*Description* |
|---------------|--------------|------------------|------------------------------|
| `actuator` | `slide` | motor | æ§åˆ¶å°è½¦ï¼ˆä½œç”¨äº slider å…³èŠ‚ï¼‰<br>*Controls cart (acts on slider joint)* |
|  | gear = `50` |  | å¢ç›Šæ”¾å¤§å€æ•°<br>*Gain amplification factor* |
|  | ctrlrange = `[-1, 1]` |  | æ§åˆ¶èŒƒå›´ï¼ˆå®é™…åŠ›ï¼šÂ±50Nï¼‰<br>*Control range (actual force: Â±50N)* |
| `sensor` | `accelerometer` | on `cart sensor` | å°è½¦åŠ é€Ÿåº¦<br>*Cart acceleration* |
|  | `touch` | on `cart sensor` | ç¢°æ’æ£€æµ‹<br>*Collision detection* |

## ğŸŸ  è‡´è°¢ä¸å¼•ç”¨ / Acknowledgments and Citations

æœ¬é¡¹ç›®åŸºäº DeepMind å¼€æºçš„ç‰©ç†å¼•æ“å¹³å° [dm_control](https://github.com/google-deepmind/dm_control) æ„å»ºï¼Œä½¿ç”¨å…¶ä¸­çš„ `cartpole` ç¯å¢ƒè¿›è¡Œå€’ç«‹æ‘†ä»¿çœŸå’Œæ§åˆ¶å™¨æµ‹è¯•ã€‚

*This project is built based on DeepMind's open-source physics engine platform [dm_control](https://github.com/google-deepmind/dm_control), using its `cartpole` environment for inverted pendulum simulation and controller testing.*

æ„Ÿè°¢è¯¥é¡¹ç›®æä¾›äº†é«˜ç²¾åº¦çš„ MuJoCo å°è£…æ¥å£ä¸æ ‡å‡†ä»»åŠ¡é›†ã€‚

*Thanks to this project for providing high-precision MuJoCo wrapper interfaces and standard task sets.*

ä»“åº“åœ°å€ / *Repository*ï¼š[https://github.com/google-deepmind/dm_control](https://github.com/google-deepmind/dm_control)

---

<div align="center">
<p><strong>ğŸ”¬ æŠ€æœ¯æ ˆ / Tech Stack</strong></p>
<p><code>Python</code> â€¢ <code>OpenAI Gym</code> â€¢ <code>Q-Learning</code> â€¢ <code>PID Control</code> â€¢ <code>MuJoCo</code> â€¢ <code>dm_control</code> â€¢ <code>MATLAB</code></p>
</div>
