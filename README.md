# CartPole-v1 Q-Learning Agent

本项目实现了一个使用 Q-learning 算法训练的强化学习智能体，用于解决 OpenAI Gym 中的经典平衡控制任务 **CartPole-v1**。项目支持人类可视化操作及完整的训练流程。

## 环境简介

CartPole 是一个倒立摆环境，其任务是通过移动小车来保持杆子竖直。游戏会在以下条件下终止：

- 杆子倾角超过 12°。
- 小车位置超出 ±2.4 范围（即小车中心到达显示屏边缘）。
- 单局游戏步数超过 200。

当连续 100 局游戏的平均得分 ≥ 195 时，认为问题已“**解决**”。

## 算法：Q-Learning（离散化）

由于环境状态是连续的，代码中通过分箱方法将状态离散化为 256 个状态（4 个特征各分 4 个区间，\(4^4 = 256\) 种组合），进而构建一个 \(256 \times 2\) 的 Q 表来存储状态-动作对的估值。

### 状态离散化

每个状态由以下 4 个连续特征组成：

- **小车位置**（cart position）
- **小车速度**（cart velocity）
- **杆子角度**（pole angle）
- **杆子角速度**（pole velocity）

每个特征值均被分为 4 个区间后离散化，最终将 4 个离散值组合成一个 0 ～ 255 的整数。

## 控制策略：ε-贪婪方法

为平衡探索与利用，采用 ε-贪婪策略：

- **ε** 随着训练进程逐步衰减：  
  \(\epsilon = 0.5 \times 0.99^{\text{episode}}\)
- 在每次决策中，以 \(1-\epsilon\) 的概率选择当前 Q 表中最优动作，以 ε 的概率随机选择动作。

## 学习过程

- **学习率**：\(\alpha = 0.2\)
- **折扣因子**：\(\gamma = 0.99\)
- 每局游戏最多 **200** 步
- 总训练局数为 **1000** 局

## 奖励机制

- 正常情况下，每一步的奖励为 1 分。
- 如果提前失败（杆子掉倒或小车超出边界），给予 **-200** 的惩罚，帮助模型更快纠正不良决策。

## 成功条件与退出机制

当连续 100 局游戏的平均得分达到或超过 195 时，训练提前终止，并输出成功提示信息：

